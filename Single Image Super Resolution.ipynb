{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from numpy import array\n",
    "\n",
    "from keras.layers import Activation, BatchNormalization, Conv2D\n",
    "from keras.models import Model\n",
    "# from keras.layers import \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "HR and LR images in the form of a numpy array from a given list of images.\n",
    "\n",
    "Low resolution images are down scaled by 4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hr_images(images):\n",
    "    images_hr = array(images)\n",
    "    return images_hr\n",
    "\n",
    "def lr_images(images_real, downscale):\n",
    "    images = []\n",
    "    for img in range(len(images_real)):\n",
    "        images.append(cv2.resize(images_real[img], [images_real[img].shape[0]//downscale, images_real[img].shape[1]//downscale], inter='bicubic', mode = None))\n",
    "    images_lr = array(images)\n",
    "    return images_lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Network\n",
    "Number of Residual blocks used are 16.\n",
    "\n",
    "Number of up-sampling blocks are 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_block_gen(model, kernel_size, filters, strides):\n",
    " \n",
    "    generator = model\n",
    "    \n",
    "    generator.add(Conv2D(filters, kernel_size = kernel_size, filters = filters, strides = strides, padding = 'same'))\n",
    "    generator.add(BatchNormalization(momentum = 0.5))\n",
    "    generator.add(PReLU(alpha_initializer = \"zeros\", alpha_regularizer = None, alpha_constraint = None, shared_axes=[1,2]))\n",
    "    generator.add(Conv2D(filters, kernel_size = kernel_size, filters = filters, strides = strides, padding = 'same'))\n",
    "    generator.add(BatchNormalization(momentum = 0.5))\n",
    "    \n",
    "    return generator\n",
    "\n",
    "def up_sampling_block(model, kernel_size, filters, strides):\n",
    "    \n",
    "    model.add(Conv2D(filters, kernel_size = kernel_size, filters = filters, strides = strides, padding = 'same'))\n",
    "    model.add(UpSampling(size = 2))\n",
    "    model.add(LeakyRelU(alpha = 0.2))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "class Generator(object):\n",
    "    \n",
    "    def __init__(self, noise_shape):\n",
    "        self.noise_shape = noise_shape\n",
    "        \n",
    "    def generator(self):\n",
    "        gen_input = Input(shape = self.noise_shape)\n",
    "        \n",
    "        model = gen_input.add(Conv2D(filters = 64, kernel_size = 9, strides = 1, padding = \"same\"))\n",
    "        model.add(PReLU(alpha_initializers = 'zeros', alpha_regularizer = None, shared_axes = [1,2] ))\n",
    "        \n",
    "        gen_model = model\n",
    "        \n",
    "        for index in range(16):\n",
    "            model = res_block_gen(model, 3, 64, 1)\n",
    "            \n",
    "        model.add(Conv2D(filters = 64, kernel_size =3, strides = 1, padding = \"same\"))\n",
    "        model.add(BatchNormalization(momentum = 0.5))\n",
    "        \n",
    "        model.add(gen_input)\n",
    "        \n",
    "        for index in range(2):\n",
    "            model = up_smapling_block(model, 3, 256, 1)\n",
    "                \n",
    "        model.add(Conv2D(filters = 64, kernel_size =9, strides = 1, padding = \"same\"))\n",
    "        model.add(Activation('tanh'))\n",
    "        \n",
    "        generator_model = Model(inputs = gen_input, outputs = model)\n",
    "        \n",
    "        return generator_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Discriminator is use to distinguish the HR images and back-propagate the GAN loss to train the discriminator and the generator'''\n",
    "\n",
    "def discriminator_block(model, filters, kernel_size, strides):\n",
    "    \n",
    "    model.add(Conv2D(filters, kernel_size = kernel_size, strides = strides, padding = 'same'))\n",
    "    model.add(BatchNormalization(momentum = 0.5))\n",
    "    model.add(LeakyRelU(alpha = 0.2))\n",
    "    \n",
    "    return model\n",
    "\n",
    "class Discriminator(object):\n",
    "    \n",
    "    def __init__(self, image_shape):\n",
    "        self.image_shape = image_shape\n",
    "        \n",
    "    def Discriminator(self):\n",
    "        \n",
    "        dis_input = Input(shape = self.image_shape)\n",
    "        \n",
    "        model = dis_input.add(Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = \"same\"))\n",
    "        model.add(LeakyReLU(alpha = 0.2))\n",
    "        \n",
    "        model = discriminator_block(model, 64, 3, 2)\n",
    "        model = discriminator_block(model, 128, 3, 1)\n",
    "        model = discriminator_block(model, 128, 3, 2)\n",
    "        model = discriminator_block(model, 256, 3, 1)\n",
    "        model = discriminator_block(model, 256, 3, 2)\n",
    "        model = discriminator_block(model, 512, 3, 1)\n",
    "        model = discriminator_block(model, 512, 3, 2)\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha = 0.2))\n",
    "        \n",
    "        model.add(Dense(1))\n",
    "        model.Activation('sigmoid')\n",
    "        \n",
    "        discriminator_model = Model(inputs = dis_input, outputs = model)\n",
    "        \n",
    "        return discriminator_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Loss Function\n",
    "\n",
    "Compares the outputs of the first convolutionns of VGG.\n",
    "\n",
    "This loss ensures the GAN model is oriented towards a deblurring task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " def vgg_loss(self, y_true, y_pred):\n",
    "        \n",
    "        vgg19 = VGG19(include_top = False, weights = \"imagenet\", input_shape = self.image_shape)\n",
    "        vgg19.trainable = False\n",
    "        \n",
    "        for l in vgg19.layers:\n",
    "            l.trainable = False\n",
    "        \n",
    "        loss_model = Model(inputs = vgg19.input, outputs = vgg19.get_layer('block5_conv4').output)\n",
    "        loss_model.trainable = False\n",
    "        \n",
    "'''vgg_loss here represents the MSE(Mean Square Error) of features extracted by a VGG-19 network. For a \n",
    "   specific layer within VGG-19, we want their features to be matched (Minimum MSE for features)'''\n",
    "       \n",
    "        vgg_loss = K.mean(K.square(loss_model(y_true) - loss_model(y_pred)))\n",
    "        return vgg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator and Discriminator combined\n",
    "Two loss used, content loss defined above and Adversarial loss(Binary cross-entropy loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gan_network(discriminator, shape, generator, optimizer, vgg_loss):\n",
    "    \n",
    "    discriminator.trainable = False #Why is the disciminator parameters not to be trained?\n",
    "    \n",
    "    gan_input = Input(shape = shape)#Why is here this input?\n",
    "    x = generator(gan_input)\n",
    "    gan_output = discriminator(x)\n",
    "    gan = Model(inputs = gan_input, outputs=[x,gan_output])\n",
    "    #Compiled GAN network with VGG loss and binary crossentropy loss\n",
    "    gan.compile(loss = [vggloss, \"binary_crossentropy\"], loss_weights = [1., 1e-3], optimizer = optimizer )\n",
    "    \n",
    "    return gan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer Function\n",
    "Adam optimizer with *B1* = 0.9, *B2* = 0.999  and learning rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer():\n",
    "    \n",
    "    adam = Adam(lr = 0.0001, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08) \n",
    "    return adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
